# Auto-scaling Configuration for ML Components
#
# This file defines three different auto-scaling configurations:
# 1. Queue-based scaling - Scales workers based on RabbitMQ queue length
# 2. Token-based scaling - Scales workers based on token processing rate
# 3. CPU-based scaling - Scales ML service based on CPU utilization

#-----------------------------------------------------------------------
# QUEUE-BASED SCALING
# Scales workers up when requests are building up in the message queue
#-----------------------------------------------------------------------
apiVersion: keda.sh/v1alpha1
kind: ScaledObject
metadata:
  name: ml-worker-scaler
  namespace: default
spec:
  # Target the Benthos ML worker deployment
  scaleTargetRef:
    apiVersion: apps/v1
    kind: Deployment
    name: benthos-ml-worker
  
  # Scaling settings
  pollingInterval: 15        # Check every 15 seconds
  cooldownPeriod: 30         # Wait 30 seconds before scaling down
  minReplicaCount: 1         # Minimum number of replicas
  maxReplicaCount: 20        # Maximum number of replicas
  
  # What triggers scaling
  triggers:
  - type: rabbitmq
    metadata:
      # Scale based on number of messages in queue
      mode: QueueLength
      queueName: ml_requests
      hostFromEnv: RABBITMQ_HOST
      protocol: amqp
      value: "5"  # Scale up when there are 5+ messages in queue
    # Authentication for RabbitMQ
    authenticationRef:
      name: rabbitmq-trigger-auth

#-----------------------------------------------------------------------
# RABBITMQ AUTHENTICATION CREDENTIALS
# Used by KEDA to connect to RabbitMQ for metrics
#-----------------------------------------------------------------------
---
apiVersion: keda.sh/v1alpha1
kind: TriggerAuthentication
metadata:
  name: rabbitmq-trigger-auth
  namespace: default
spec:
  # Pull credentials from Secret
  secretTargetRef:
  - parameter: host
    name: rabbitmq-connection
    key: host
  - parameter: username
    name: rabbitmq-connection
    key: username
  - parameter: password
    name: rabbitmq-connection
    key: password

#-----------------------------------------------------------------------
# RABBITMQ CONNECTION SECRET
# Contains connection details for RabbitMQ scaling trigger
#-----------------------------------------------------------------------
---
apiVersion: v1
kind: Secret
metadata:
  name: rabbitmq-connection
  namespace: default
type: Opaque
stringData:
  # Connection info for KEDA RabbitMQ scaler
  host: amqp://rabbitmq.data-infra.svc.cluster.local:5672
  username: user
  password: your-secure-password

#-----------------------------------------------------------------------
# TOKEN-BASED SCALING
# Scales workers up when token processing rate increases
#-----------------------------------------------------------------------
---
apiVersion: keda.sh/v1alpha1
kind: ScaledObject
metadata:
  name: ml-worker-token-scaler
  namespace: default
spec:
  # Target the Benthos ML worker deployment
  scaleTargetRef:
    name: benthos-ml-worker
    kind: Deployment
  
  # Scaling settings
  minReplicaCount: 1         # Minimum number of replicas
  maxReplicaCount: 10        # Maximum number of replicas
  pollingInterval: 15        # Check every 15 seconds
  cooldownPeriod: 30         # Wait 30 seconds before scaling down
  
  # What triggers scaling
  triggers:
    - type: prometheus
      metadata:
        # Get metrics from Prometheus
        serverAddress: http://prometheus-server.monitoring.svc.cluster.local
        metricName: ml_tokens_processed_rate
        # Tokens processed per minute (calculated from 5-minute rate)
        query: sum(rate(ml_tokens_processed_total{type="prompt"}[5m])) * 60
        threshold: "5000"    # Scale up when processing >5000 tokens/minute

#-----------------------------------------------------------------------
# CPU-BASED SCALING
# Scales ML service up when CPU utilization increases
#-----------------------------------------------------------------------
---
apiVersion: keda.sh/v1alpha1
kind: ScaledObject
metadata:
  name: ml-service-scaler
  namespace: default
spec:
  # Target the ML inference service deployment
  scaleTargetRef:
    name: ml-inference
    kind: Deployment
  
  # Scaling settings
  minReplicaCount: 1         # Minimum number of replicas
  maxReplicaCount: 5         # Maximum number of replicas 
  pollingInterval: 15        # Check every 15 seconds
  cooldownPeriod: 30         # Wait 30 seconds before scaling down
  
  # What triggers scaling
  triggers:
    - type: cpu
      metadata:
        type: Utilization
        value: "70"          # Scale up when CPU usage >70% 