input:
  type: amqp
  amqp:
    url: "amqp://prod_user:StrongProdPassw0rd!@rabbitmq.queue.svc.cluster.local:5672/"
    queue: "inference_requests"
    # Optional: Set to 1 to process one message at a time
    prefetch_count: 1

pipeline:
  processors:
    # Forward the message to the ML service over HTTP
    - http_client:
        url: "http://llm-service.queue.svc.cluster.local:8080/inference"
        method: "POST"
        headers:
          Content-Type: application/json
        body: |
          {
            "input": "${! content() }"
          }
        # Expect a JSON response with the prediction result
        response_map:
          result: "body.result"
        # Optional: Set to true if the ML service streams responses
        stream: false

    # Optional: Add any additional processors here (e.g., logging, error handling)

output:
  type: amqp
  amqp:
    url: "amqp://prod_user:StrongProdPassw0rd!@rabbitmq.queue.svc.cluster.local:5672/"
    queue: "inference_results"
    publish_mode: "persistent"
